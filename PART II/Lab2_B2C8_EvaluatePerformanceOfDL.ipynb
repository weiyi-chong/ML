{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab2_B2C8_EvaluatePerformanceOfDL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6aXt60imzJIqytCZWDBQq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Evaluate The Performance of Deep Learning Models\n","After completing this lesson, you will know:\n","* How to evaluate a Keras model using an automatic verification dataset.\n","* How to evaluate a Keras model using a manual verification dataset.\n","* How to evaluate a Keras model using k-fold cross-validation."],"metadata":{"id":"gwRDZ4JVsfVg"}},{"cell_type":"markdown","source":["##Data Splitting\n","Keras provides two convenient ways of evaluating your deep learning algorithms this way:\n","1. Use an automatic verification dataset.\n","2. Use a manual verification dataset."],"metadata":{"id":"d1_QEwposyAr"}},{"cell_type":"markdown","source":["### Automatic Verification Dataset\n","You can do this by setting the **validation split** argument on the fit() function to a percentage of the size of your training dataset."],"metadata":{"id":"E_HJw-Sks3xY"}},{"cell_type":"code","source":["# MLP with automatic validation set\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import numpy\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","\n","# load pima indians dataset\n","dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n","\n","# split into input (X) and output (Y) variables\n","X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","# create model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation= 'relu' ))\n","model.add(Dense(8, activation= 'relu' ))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","\n","# Compile model\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","\n","# Fit the model\n","model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ym8MvWXqse02","executionInfo":{"status":"ok","timestamp":1653558142340,"user_tz":-480,"elapsed":30671,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"947bc69b-b274-4e31-c496-9d37cbd58ba8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","52/52 [==============================] - 1s 6ms/step - loss: 21.0607 - accuracy: 0.3599 - val_loss: 9.2149 - val_accuracy: 0.3701\n","Epoch 2/150\n","52/52 [==============================] - 0s 3ms/step - loss: 4.4093 - accuracy: 0.5078 - val_loss: 2.3305 - val_accuracy: 0.5315\n","Epoch 3/150\n","52/52 [==============================] - 0s 4ms/step - loss: 1.8864 - accuracy: 0.4961 - val_loss: 1.4704 - val_accuracy: 0.4646\n","Epoch 4/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.4183 - accuracy: 0.4825 - val_loss: 1.3045 - val_accuracy: 0.5236\n","Epoch 5/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.2985 - accuracy: 0.4805 - val_loss: 1.2831 - val_accuracy: 0.4134\n","Epoch 6/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.1403 - accuracy: 0.5097 - val_loss: 1.1387 - val_accuracy: 0.4724\n","Epoch 7/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.0596 - accuracy: 0.4981 - val_loss: 1.0585 - val_accuracy: 0.4882\n","Epoch 8/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.0110 - accuracy: 0.5389 - val_loss: 0.9569 - val_accuracy: 0.5591\n","Epoch 9/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.9473 - accuracy: 0.5506 - val_loss: 0.9558 - val_accuracy: 0.4803\n","Epoch 10/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.9062 - accuracy: 0.5661 - val_loss: 0.8607 - val_accuracy: 0.5551\n","Epoch 11/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.8943 - accuracy: 0.5603 - val_loss: 0.8982 - val_accuracy: 0.6654\n","Epoch 12/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.8684 - accuracy: 0.5837 - val_loss: 0.7844 - val_accuracy: 0.6102\n","Epoch 13/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7958 - accuracy: 0.6284 - val_loss: 0.7748 - val_accuracy: 0.5630\n","Epoch 14/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7653 - accuracy: 0.6245 - val_loss: 0.7378 - val_accuracy: 0.6575\n","Epoch 15/150\n","52/52 [==============================] - 0s 8ms/step - loss: 0.7499 - accuracy: 0.6089 - val_loss: 0.7164 - val_accuracy: 0.6457\n","Epoch 16/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7254 - accuracy: 0.6401 - val_loss: 0.7220 - val_accuracy: 0.5827\n","Epoch 17/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7107 - accuracy: 0.6304 - val_loss: 0.6806 - val_accuracy: 0.6575\n","Epoch 18/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7011 - accuracy: 0.6342 - val_loss: 0.6833 - val_accuracy: 0.6732\n","Epoch 19/150\n","52/52 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.6595 - val_loss: 0.6942 - val_accuracy: 0.5906\n","Epoch 20/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.6576 - val_loss: 0.6540 - val_accuracy: 0.6614\n","Epoch 21/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.6673 - val_loss: 0.6515 - val_accuracy: 0.6378\n","Epoch 22/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.6829 - val_loss: 0.6596 - val_accuracy: 0.6220\n","Epoch 23/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6770 - val_loss: 0.6512 - val_accuracy: 0.6260\n","Epoch 24/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6553 - accuracy: 0.6693 - val_loss: 0.6266 - val_accuracy: 0.6654\n","Epoch 25/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6359 - accuracy: 0.6615 - val_loss: 0.6580 - val_accuracy: 0.6220\n","Epoch 26/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.6345 - accuracy: 0.6809 - val_loss: 0.6682 - val_accuracy: 0.6378\n","Epoch 27/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6848 - val_loss: 0.6198 - val_accuracy: 0.6732\n","Epoch 28/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6693 - val_loss: 0.6118 - val_accuracy: 0.6575\n","Epoch 29/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6907 - val_loss: 0.6951 - val_accuracy: 0.6732\n","Epoch 30/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.6732 - val_loss: 0.6057 - val_accuracy: 0.6969\n","Epoch 31/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.6868 - val_loss: 0.6060 - val_accuracy: 0.6457\n","Epoch 32/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.6164 - accuracy: 0.6887 - val_loss: 0.6440 - val_accuracy: 0.6102\n","Epoch 33/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.6790 - val_loss: 0.7307 - val_accuracy: 0.5591\n","Epoch 34/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6790 - val_loss: 0.6037 - val_accuracy: 0.6339\n","Epoch 35/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.6809 - val_loss: 0.6260 - val_accuracy: 0.6772\n","Epoch 36/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.6732 - val_loss: 0.6412 - val_accuracy: 0.6220\n","Epoch 37/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.6848 - val_loss: 0.6044 - val_accuracy: 0.6378\n","Epoch 38/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5870 - accuracy: 0.7043 - val_loss: 0.5902 - val_accuracy: 0.6969\n","Epoch 39/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5862 - accuracy: 0.7043 - val_loss: 0.6120 - val_accuracy: 0.6417\n","Epoch 40/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6040 - accuracy: 0.6984 - val_loss: 0.5988 - val_accuracy: 0.6850\n","Epoch 41/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5874 - accuracy: 0.7101 - val_loss: 0.6133 - val_accuracy: 0.6142\n","Epoch 42/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5950 - accuracy: 0.7043 - val_loss: 0.5916 - val_accuracy: 0.6535\n","Epoch 43/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6770 - val_loss: 0.6310 - val_accuracy: 0.6969\n","Epoch 44/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7335 - val_loss: 0.5891 - val_accuracy: 0.6654\n","Epoch 45/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5926 - accuracy: 0.6907 - val_loss: 0.6197 - val_accuracy: 0.6378\n","Epoch 46/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5781 - accuracy: 0.7276 - val_loss: 0.5908 - val_accuracy: 0.6772\n","Epoch 47/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7179 - val_loss: 0.5798 - val_accuracy: 0.6811\n","Epoch 48/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7101 - val_loss: 0.6687 - val_accuracy: 0.5945\n","Epoch 49/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5713 - accuracy: 0.7198 - val_loss: 0.5832 - val_accuracy: 0.6929\n","Epoch 50/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6313 - accuracy: 0.7043 - val_loss: 0.5925 - val_accuracy: 0.6614\n","Epoch 51/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.7179 - val_loss: 0.6404 - val_accuracy: 0.6142\n","Epoch 52/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5716 - accuracy: 0.7237 - val_loss: 0.5769 - val_accuracy: 0.6811\n","Epoch 53/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5874 - accuracy: 0.7140 - val_loss: 0.5914 - val_accuracy: 0.6654\n","Epoch 54/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5748 - accuracy: 0.6965 - val_loss: 0.5788 - val_accuracy: 0.7008\n","Epoch 55/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.7218 - val_loss: 0.6282 - val_accuracy: 0.6260\n","Epoch 56/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5649 - accuracy: 0.7237 - val_loss: 0.5875 - val_accuracy: 0.6732\n","Epoch 57/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5570 - accuracy: 0.7374 - val_loss: 0.5837 - val_accuracy: 0.6693\n","Epoch 58/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.7179 - val_loss: 0.5721 - val_accuracy: 0.6969\n","Epoch 59/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5582 - accuracy: 0.7101 - val_loss: 0.5769 - val_accuracy: 0.6732\n","Epoch 60/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5635 - accuracy: 0.7218 - val_loss: 0.5692 - val_accuracy: 0.7008\n","Epoch 61/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.7179 - val_loss: 0.6015 - val_accuracy: 0.6850\n","Epoch 62/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5684 - accuracy: 0.7237 - val_loss: 0.5700 - val_accuracy: 0.7047\n","Epoch 63/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.7179 - val_loss: 0.5800 - val_accuracy: 0.6772\n","Epoch 64/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5653 - accuracy: 0.7315 - val_loss: 0.5692 - val_accuracy: 0.7047\n","Epoch 65/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5668 - accuracy: 0.6907 - val_loss: 0.5782 - val_accuracy: 0.6890\n","Epoch 66/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5643 - accuracy: 0.7179 - val_loss: 0.5852 - val_accuracy: 0.6772\n","Epoch 67/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.7023 - val_loss: 0.5662 - val_accuracy: 0.7087\n","Epoch 68/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5549 - accuracy: 0.7237 - val_loss: 0.5619 - val_accuracy: 0.6929\n","Epoch 69/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5647 - accuracy: 0.7082 - val_loss: 0.5701 - val_accuracy: 0.6929\n","Epoch 70/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5501 - accuracy: 0.7218 - val_loss: 0.5819 - val_accuracy: 0.7165\n","Epoch 71/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.7510 - val_loss: 0.5603 - val_accuracy: 0.7205\n","Epoch 72/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5553 - accuracy: 0.7101 - val_loss: 0.5841 - val_accuracy: 0.6732\n","Epoch 73/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7218 - val_loss: 0.5585 - val_accuracy: 0.7047\n","Epoch 74/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5748 - accuracy: 0.7043 - val_loss: 0.5683 - val_accuracy: 0.6969\n","Epoch 75/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7276 - val_loss: 0.5602 - val_accuracy: 0.7205\n","Epoch 76/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.7004 - val_loss: 0.5700 - val_accuracy: 0.7087\n","Epoch 77/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5932 - accuracy: 0.7257 - val_loss: 0.6408 - val_accuracy: 0.7047\n","Epoch 78/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.7237 - val_loss: 0.5643 - val_accuracy: 0.6969\n","Epoch 79/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5509 - accuracy: 0.7315 - val_loss: 0.5519 - val_accuracy: 0.7165\n","Epoch 80/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5589 - accuracy: 0.7354 - val_loss: 0.5673 - val_accuracy: 0.7087\n","Epoch 81/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.7004 - val_loss: 0.5652 - val_accuracy: 0.7087\n","Epoch 82/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5513 - accuracy: 0.7140 - val_loss: 0.5636 - val_accuracy: 0.7126\n","Epoch 83/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.7315 - val_loss: 0.5629 - val_accuracy: 0.7008\n","Epoch 84/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.7354 - val_loss: 0.5517 - val_accuracy: 0.7205\n","Epoch 85/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.7412 - val_loss: 0.5564 - val_accuracy: 0.7165\n","Epoch 86/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5638 - accuracy: 0.7140 - val_loss: 0.6061 - val_accuracy: 0.6535\n","Epoch 87/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5383 - accuracy: 0.7374 - val_loss: 0.5527 - val_accuracy: 0.7047\n","Epoch 88/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5503 - accuracy: 0.7237 - val_loss: 0.5569 - val_accuracy: 0.7323\n","Epoch 89/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5425 - accuracy: 0.7393 - val_loss: 0.6002 - val_accuracy: 0.6614\n","Epoch 90/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5604 - accuracy: 0.7354 - val_loss: 0.5527 - val_accuracy: 0.7087\n","Epoch 91/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7432 - val_loss: 0.5875 - val_accuracy: 0.6614\n","Epoch 92/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6121 - accuracy: 0.6907 - val_loss: 0.5517 - val_accuracy: 0.7402\n","Epoch 93/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.7023 - val_loss: 0.5588 - val_accuracy: 0.7087\n","Epoch 94/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.7374 - val_loss: 0.5466 - val_accuracy: 0.7087\n","Epoch 95/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5588 - accuracy: 0.7023 - val_loss: 0.5706 - val_accuracy: 0.6969\n","Epoch 96/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5670 - accuracy: 0.7218 - val_loss: 0.5479 - val_accuracy: 0.7244\n","Epoch 97/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5399 - accuracy: 0.7315 - val_loss: 0.5493 - val_accuracy: 0.7362\n","Epoch 98/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5449 - accuracy: 0.7315 - val_loss: 0.5562 - val_accuracy: 0.7244\n","Epoch 99/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7276 - val_loss: 0.5497 - val_accuracy: 0.7205\n","Epoch 100/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.7490 - val_loss: 0.5590 - val_accuracy: 0.7047\n","Epoch 101/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5362 - accuracy: 0.7432 - val_loss: 0.5430 - val_accuracy: 0.7402\n","Epoch 102/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.7296 - val_loss: 0.5480 - val_accuracy: 0.7402\n","Epoch 103/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5377 - accuracy: 0.7432 - val_loss: 0.5428 - val_accuracy: 0.7362\n","Epoch 104/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.7237 - val_loss: 0.5702 - val_accuracy: 0.7087\n","Epoch 105/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5415 - accuracy: 0.7335 - val_loss: 0.5687 - val_accuracy: 0.7047\n","Epoch 106/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7315 - val_loss: 0.5513 - val_accuracy: 0.7205\n","Epoch 107/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7549 - val_loss: 0.5362 - val_accuracy: 0.7362\n","Epoch 108/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.7315 - val_loss: 0.5397 - val_accuracy: 0.7559\n","Epoch 109/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5417 - accuracy: 0.7315 - val_loss: 0.5418 - val_accuracy: 0.7205\n","Epoch 110/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5607 - accuracy: 0.7160 - val_loss: 0.6363 - val_accuracy: 0.6299\n","Epoch 111/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7412 - val_loss: 0.5482 - val_accuracy: 0.7165\n","Epoch 112/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.7529 - val_loss: 0.5330 - val_accuracy: 0.7362\n","Epoch 113/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7412 - val_loss: 0.5498 - val_accuracy: 0.7480\n","Epoch 114/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.7335 - val_loss: 0.5415 - val_accuracy: 0.7559\n","Epoch 115/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.7510 - val_loss: 0.5415 - val_accuracy: 0.7165\n","Epoch 116/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5401 - accuracy: 0.7354 - val_loss: 0.5360 - val_accuracy: 0.7244\n","Epoch 117/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7451 - val_loss: 0.5302 - val_accuracy: 0.7323\n","Epoch 118/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7471 - val_loss: 0.5544 - val_accuracy: 0.7283\n","Epoch 119/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7549 - val_loss: 0.5450 - val_accuracy: 0.7323\n","Epoch 120/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7315 - val_loss: 0.5430 - val_accuracy: 0.7323\n","Epoch 121/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7510 - val_loss: 0.5285 - val_accuracy: 0.7283\n","Epoch 122/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5571 - accuracy: 0.7082 - val_loss: 0.5422 - val_accuracy: 0.7283\n","Epoch 123/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.7471 - val_loss: 0.5328 - val_accuracy: 0.7205\n","Epoch 124/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5298 - accuracy: 0.7393 - val_loss: 0.5346 - val_accuracy: 0.7323\n","Epoch 125/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5319 - accuracy: 0.7451 - val_loss: 0.5384 - val_accuracy: 0.7402\n","Epoch 126/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.7432 - val_loss: 0.5655 - val_accuracy: 0.7087\n","Epoch 127/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5500 - accuracy: 0.7257 - val_loss: 0.5310 - val_accuracy: 0.7559\n","Epoch 128/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7510 - val_loss: 0.5578 - val_accuracy: 0.7283\n","Epoch 129/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7335 - val_loss: 0.5685 - val_accuracy: 0.7126\n","Epoch 130/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5457 - accuracy: 0.7257 - val_loss: 0.5267 - val_accuracy: 0.7323\n","Epoch 131/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7412 - val_loss: 0.5465 - val_accuracy: 0.7205\n","Epoch 132/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5490 - accuracy: 0.7315 - val_loss: 0.5239 - val_accuracy: 0.7559\n","Epoch 133/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5302 - accuracy: 0.7412 - val_loss: 0.5342 - val_accuracy: 0.7598\n","Epoch 134/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 0.7374 - val_loss: 0.5204 - val_accuracy: 0.7402\n","Epoch 135/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7374 - val_loss: 0.5466 - val_accuracy: 0.7244\n","Epoch 136/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7529 - val_loss: 0.6178 - val_accuracy: 0.6496\n","Epoch 137/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7393 - val_loss: 0.5479 - val_accuracy: 0.7283\n","Epoch 138/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7374 - val_loss: 0.5190 - val_accuracy: 0.7520\n","Epoch 139/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5234 - accuracy: 0.7549 - val_loss: 0.5230 - val_accuracy: 0.7402\n","Epoch 140/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7412 - val_loss: 0.5284 - val_accuracy: 0.7795\n","Epoch 141/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7296 - val_loss: 0.5994 - val_accuracy: 0.7047\n","Epoch 142/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.7315 - val_loss: 0.5170 - val_accuracy: 0.7598\n","Epoch 143/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.7198 - val_loss: 0.5362 - val_accuracy: 0.7677\n","Epoch 144/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5217 - accuracy: 0.7432 - val_loss: 0.5359 - val_accuracy: 0.7598\n","Epoch 145/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7626 - val_loss: 0.5140 - val_accuracy: 0.7559\n","Epoch 146/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5378 - accuracy: 0.7393 - val_loss: 0.5210 - val_accuracy: 0.7598\n","Epoch 147/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7296 - val_loss: 0.5223 - val_accuracy: 0.7244\n","Epoch 148/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 0.7315 - val_loss: 0.5451 - val_accuracy: 0.7441\n","Epoch 149/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5391 - accuracy: 0.7296 - val_loss: 0.5406 - val_accuracy: 0.7441\n","Epoch 150/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.7335 - val_loss: 0.5171 - val_accuracy: 0.7638\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f95b843f610>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["### Manual Verification Dataset\n","**train test split()** function from the Python scikit-learn\n","machine learning library to separate our data into a training and test dataset. We use 67% for training and the remaining 33% of the data for validation"],"metadata":{"id":"e6aD0BUIs-GX"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9x9MPEUsRPU","executionInfo":{"status":"ok","timestamp":1653558168842,"user_tz":-480,"elapsed":26512,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"7fab6e2e-4de4-4f55-9752-063c7ae86ebb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","52/52 [==============================] - 1s 6ms/step - loss: 8.7145 - accuracy: 0.6518 - val_loss: 2.5341 - val_accuracy: 0.4961\n","Epoch 2/150\n","52/52 [==============================] - 0s 3ms/step - loss: 2.0011 - accuracy: 0.4922 - val_loss: 1.3285 - val_accuracy: 0.5748\n","Epoch 3/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.4213 - accuracy: 0.5545 - val_loss: 1.1369 - val_accuracy: 0.5551\n","Epoch 4/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.2510 - accuracy: 0.5428 - val_loss: 1.0965 - val_accuracy: 0.6496\n","Epoch 5/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.1150 - accuracy: 0.6226 - val_loss: 0.9513 - val_accuracy: 0.6457\n","Epoch 6/150\n","52/52 [==============================] - 0s 3ms/step - loss: 1.0161 - accuracy: 0.6031 - val_loss: 0.9168 - val_accuracy: 0.6732\n","Epoch 7/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.9266 - accuracy: 0.6089 - val_loss: 0.8291 - val_accuracy: 0.6890\n","Epoch 8/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.8541 - accuracy: 0.6284 - val_loss: 0.7530 - val_accuracy: 0.6654\n","Epoch 9/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.8007 - accuracy: 0.6284 - val_loss: 0.7587 - val_accuracy: 0.6535\n","Epoch 10/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7623 - accuracy: 0.6245 - val_loss: 0.6910 - val_accuracy: 0.6890\n","Epoch 11/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.7029 - accuracy: 0.6420 - val_loss: 0.6785 - val_accuracy: 0.6890\n","Epoch 12/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6734 - accuracy: 0.6712 - val_loss: 0.6731 - val_accuracy: 0.6654\n","Epoch 13/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6751 - val_loss: 0.6711 - val_accuracy: 0.6496\n","Epoch 14/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6790 - val_loss: 0.6569 - val_accuracy: 0.6457\n","Epoch 15/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6354 - accuracy: 0.6829 - val_loss: 0.7237 - val_accuracy: 0.6457\n","Epoch 16/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6431 - accuracy: 0.6459 - val_loss: 0.6565 - val_accuracy: 0.6260\n","Epoch 17/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6079 - accuracy: 0.6712 - val_loss: 0.6541 - val_accuracy: 0.6496\n","Epoch 18/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6926 - val_loss: 0.6753 - val_accuracy: 0.6654\n","Epoch 19/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6079 - accuracy: 0.6907 - val_loss: 0.6520 - val_accuracy: 0.6457\n","Epoch 20/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6732 - val_loss: 0.6502 - val_accuracy: 0.6654\n","Epoch 21/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5870 - accuracy: 0.6868 - val_loss: 0.6493 - val_accuracy: 0.6457\n","Epoch 22/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5946 - accuracy: 0.6907 - val_loss: 0.6964 - val_accuracy: 0.6181\n","Epoch 23/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5861 - accuracy: 0.7023 - val_loss: 0.6372 - val_accuracy: 0.6890\n","Epoch 24/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5705 - accuracy: 0.7140 - val_loss: 0.6616 - val_accuracy: 0.6417\n","Epoch 25/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5768 - accuracy: 0.7101 - val_loss: 0.6545 - val_accuracy: 0.6457\n","Epoch 26/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.6907 - val_loss: 0.6296 - val_accuracy: 0.7047\n","Epoch 27/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5824 - accuracy: 0.6984 - val_loss: 0.6359 - val_accuracy: 0.6614\n","Epoch 28/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.6907 - val_loss: 0.6496 - val_accuracy: 0.6614\n","Epoch 29/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7140 - val_loss: 0.6476 - val_accuracy: 0.6654\n","Epoch 30/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5784 - accuracy: 0.6790 - val_loss: 0.6555 - val_accuracy: 0.6575\n","Epoch 31/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7023 - val_loss: 0.6500 - val_accuracy: 0.6732\n","Epoch 32/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5422 - accuracy: 0.7354 - val_loss: 0.6452 - val_accuracy: 0.6575\n","Epoch 33/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5782 - accuracy: 0.7043 - val_loss: 0.6221 - val_accuracy: 0.7087\n","Epoch 34/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5594 - accuracy: 0.7218 - val_loss: 0.6434 - val_accuracy: 0.6614\n","Epoch 35/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5457 - accuracy: 0.7393 - val_loss: 0.8652 - val_accuracy: 0.6575\n","Epoch 36/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5691 - accuracy: 0.7140 - val_loss: 0.6215 - val_accuracy: 0.7165\n","Epoch 37/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5927 - accuracy: 0.6907 - val_loss: 0.6252 - val_accuracy: 0.7008\n","Epoch 38/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.7198 - val_loss: 0.6204 - val_accuracy: 0.7165\n","Epoch 39/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5407 - accuracy: 0.7276 - val_loss: 0.6393 - val_accuracy: 0.6732\n","Epoch 40/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5392 - accuracy: 0.7276 - val_loss: 0.6294 - val_accuracy: 0.6850\n","Epoch 41/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5560 - accuracy: 0.7276 - val_loss: 0.6222 - val_accuracy: 0.6969\n","Epoch 42/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7179 - val_loss: 0.6575 - val_accuracy: 0.6772\n","Epoch 43/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7198 - val_loss: 0.6233 - val_accuracy: 0.6969\n","Epoch 44/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5692 - accuracy: 0.7160 - val_loss: 0.6647 - val_accuracy: 0.6496\n","Epoch 45/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.7043 - val_loss: 0.7789 - val_accuracy: 0.6181\n","Epoch 46/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.6192 - accuracy: 0.6770 - val_loss: 0.6163 - val_accuracy: 0.7126\n","Epoch 47/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5456 - accuracy: 0.7354 - val_loss: 0.6261 - val_accuracy: 0.6929\n","Epoch 48/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7101 - val_loss: 0.6521 - val_accuracy: 0.6811\n","Epoch 49/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7276 - val_loss: 0.6162 - val_accuracy: 0.7165\n","Epoch 50/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.7121 - val_loss: 0.8839 - val_accuracy: 0.6575\n","Epoch 51/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5608 - accuracy: 0.7121 - val_loss: 0.6649 - val_accuracy: 0.6693\n","Epoch 52/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.7198 - val_loss: 0.6349 - val_accuracy: 0.6969\n","Epoch 53/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7335 - val_loss: 0.7041 - val_accuracy: 0.6732\n","Epoch 54/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5371 - accuracy: 0.7510 - val_loss: 0.6242 - val_accuracy: 0.6929\n","Epoch 55/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5439 - accuracy: 0.7471 - val_loss: 0.6478 - val_accuracy: 0.6890\n","Epoch 56/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5461 - accuracy: 0.7237 - val_loss: 0.6066 - val_accuracy: 0.7165\n","Epoch 57/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5429 - accuracy: 0.7276 - val_loss: 0.6560 - val_accuracy: 0.6496\n","Epoch 58/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.7354 - val_loss: 0.6177 - val_accuracy: 0.7244\n","Epoch 59/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5400 - accuracy: 0.7315 - val_loss: 0.7611 - val_accuracy: 0.6654\n","Epoch 60/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5714 - accuracy: 0.7160 - val_loss: 0.6269 - val_accuracy: 0.6890\n","Epoch 61/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7393 - val_loss: 0.6060 - val_accuracy: 0.7205\n","Epoch 62/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7257 - val_loss: 0.6704 - val_accuracy: 0.6614\n","Epoch 63/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7412 - val_loss: 0.6457 - val_accuracy: 0.6929\n","Epoch 64/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.7412 - val_loss: 0.6314 - val_accuracy: 0.6772\n","Epoch 65/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.7529 - val_loss: 0.6297 - val_accuracy: 0.7008\n","Epoch 66/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5272 - accuracy: 0.7510 - val_loss: 0.6221 - val_accuracy: 0.7008\n","Epoch 67/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5399 - accuracy: 0.7335 - val_loss: 0.6089 - val_accuracy: 0.7323\n","Epoch 68/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5639 - accuracy: 0.7179 - val_loss: 0.6133 - val_accuracy: 0.7165\n","Epoch 69/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7335 - val_loss: 0.6305 - val_accuracy: 0.6890\n","Epoch 70/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7121 - val_loss: 1.0354 - val_accuracy: 0.6654\n","Epoch 71/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7179 - val_loss: 0.6615 - val_accuracy: 0.6614\n","Epoch 72/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7237 - val_loss: 0.6398 - val_accuracy: 0.6850\n","Epoch 73/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7471 - val_loss: 0.5997 - val_accuracy: 0.7480\n","Epoch 74/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5415 - accuracy: 0.7374 - val_loss: 0.6034 - val_accuracy: 0.7323\n","Epoch 75/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7276 - val_loss: 0.6135 - val_accuracy: 0.7244\n","Epoch 76/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7626 - val_loss: 0.6025 - val_accuracy: 0.7362\n","Epoch 77/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5260 - accuracy: 0.7510 - val_loss: 0.6040 - val_accuracy: 0.7283\n","Epoch 78/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7198 - val_loss: 0.6355 - val_accuracy: 0.6969\n","Epoch 79/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5122 - accuracy: 0.7490 - val_loss: 0.6162 - val_accuracy: 0.7126\n","Epoch 80/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5141 - accuracy: 0.7490 - val_loss: 0.6085 - val_accuracy: 0.7283\n","Epoch 81/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.6984 - val_loss: 0.6208 - val_accuracy: 0.7047\n","Epoch 82/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.7335 - val_loss: 0.5992 - val_accuracy: 0.7402\n","Epoch 83/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7004 - val_loss: 0.6003 - val_accuracy: 0.7244\n","Epoch 84/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5159 - accuracy: 0.7490 - val_loss: 0.6670 - val_accuracy: 0.6614\n","Epoch 85/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5178 - accuracy: 0.7529 - val_loss: 0.6722 - val_accuracy: 0.6850\n","Epoch 86/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7315 - val_loss: 0.6063 - val_accuracy: 0.7402\n","Epoch 87/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5116 - accuracy: 0.7568 - val_loss: 0.6135 - val_accuracy: 0.7283\n","Epoch 88/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.7393 - val_loss: 0.6685 - val_accuracy: 0.6496\n","Epoch 89/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5594 - accuracy: 0.7218 - val_loss: 0.6266 - val_accuracy: 0.7008\n","Epoch 90/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5098 - accuracy: 0.7412 - val_loss: 0.5972 - val_accuracy: 0.7441\n","Epoch 91/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7607 - val_loss: 0.6350 - val_accuracy: 0.7087\n","Epoch 92/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7529 - val_loss: 0.6006 - val_accuracy: 0.7244\n","Epoch 93/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5181 - accuracy: 0.7529 - val_loss: 0.6318 - val_accuracy: 0.7126\n","Epoch 94/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7393 - val_loss: 0.6366 - val_accuracy: 0.6811\n","Epoch 95/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7490 - val_loss: 0.6237 - val_accuracy: 0.7008\n","Epoch 96/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7490 - val_loss: 0.5915 - val_accuracy: 0.7402\n","Epoch 97/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.7607 - val_loss: 0.6447 - val_accuracy: 0.6929\n","Epoch 98/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.7179 - val_loss: 0.7354 - val_accuracy: 0.6732\n","Epoch 99/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5206 - accuracy: 0.7412 - val_loss: 0.5912 - val_accuracy: 0.7402\n","Epoch 100/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5174 - accuracy: 0.7335 - val_loss: 0.6614 - val_accuracy: 0.6772\n","Epoch 101/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7549 - val_loss: 0.6003 - val_accuracy: 0.7126\n","Epoch 102/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.7393 - val_loss: 0.6204 - val_accuracy: 0.6890\n","Epoch 103/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.7490 - val_loss: 0.5905 - val_accuracy: 0.7402\n","Epoch 104/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7354 - val_loss: 0.6412 - val_accuracy: 0.6850\n","Epoch 105/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5139 - accuracy: 0.7665 - val_loss: 0.6106 - val_accuracy: 0.6850\n","Epoch 106/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4927 - accuracy: 0.7529 - val_loss: 0.6242 - val_accuracy: 0.7008\n","Epoch 107/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.7296 - val_loss: 0.6280 - val_accuracy: 0.7047\n","Epoch 108/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.5026 - accuracy: 0.7607 - val_loss: 0.5872 - val_accuracy: 0.7323\n","Epoch 109/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7510 - val_loss: 0.6101 - val_accuracy: 0.7323\n","Epoch 110/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7549 - val_loss: 0.5931 - val_accuracy: 0.7362\n","Epoch 111/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.7179 - val_loss: 0.5882 - val_accuracy: 0.7402\n","Epoch 112/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5178 - accuracy: 0.7490 - val_loss: 0.5908 - val_accuracy: 0.7205\n","Epoch 113/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7607 - val_loss: 0.6218 - val_accuracy: 0.7126\n","Epoch 114/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.6226 - val_accuracy: 0.6969\n","Epoch 115/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7588 - val_loss: 0.6174 - val_accuracy: 0.7126\n","Epoch 116/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7665 - val_loss: 0.5851 - val_accuracy: 0.7559\n","Epoch 117/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7471 - val_loss: 0.5874 - val_accuracy: 0.7480\n","Epoch 118/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5094 - accuracy: 0.7685 - val_loss: 0.5802 - val_accuracy: 0.7480\n","Epoch 119/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4934 - accuracy: 0.7607 - val_loss: 0.6126 - val_accuracy: 0.7087\n","Epoch 120/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.7568 - val_loss: 0.6106 - val_accuracy: 0.7126\n","Epoch 121/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5165 - accuracy: 0.7432 - val_loss: 0.6357 - val_accuracy: 0.6929\n","Epoch 122/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7471 - val_loss: 0.5856 - val_accuracy: 0.7441\n","Epoch 123/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5200 - accuracy: 0.7412 - val_loss: 0.6039 - val_accuracy: 0.7244\n","Epoch 124/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.7626 - val_loss: 0.6760 - val_accuracy: 0.6929\n","Epoch 125/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7529 - val_loss: 0.6005 - val_accuracy: 0.7244\n","Epoch 126/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7607 - val_loss: 0.5968 - val_accuracy: 0.7362\n","Epoch 127/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4973 - accuracy: 0.7510 - val_loss: 0.6276 - val_accuracy: 0.7165\n","Epoch 128/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7432 - val_loss: 0.6234 - val_accuracy: 0.7126\n","Epoch 129/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5107 - accuracy: 0.7549 - val_loss: 0.5871 - val_accuracy: 0.7480\n","Epoch 130/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7490 - val_loss: 0.5843 - val_accuracy: 0.7598\n","Epoch 131/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4991 - accuracy: 0.7704 - val_loss: 0.5846 - val_accuracy: 0.7559\n","Epoch 132/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4976 - accuracy: 0.7665 - val_loss: 0.5904 - val_accuracy: 0.7362\n","Epoch 133/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.7549 - val_loss: 0.5809 - val_accuracy: 0.7520\n","Epoch 134/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4966 - accuracy: 0.7626 - val_loss: 0.6041 - val_accuracy: 0.7126\n","Epoch 135/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5183 - accuracy: 0.7432 - val_loss: 0.6191 - val_accuracy: 0.7362\n","Epoch 136/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.4894 - accuracy: 0.7821 - val_loss: 0.6026 - val_accuracy: 0.7480\n","Epoch 137/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7646 - val_loss: 0.6538 - val_accuracy: 0.6811\n","Epoch 138/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.7626 - val_loss: 0.5821 - val_accuracy: 0.7480\n","Epoch 139/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7568 - val_loss: 0.5831 - val_accuracy: 0.7402\n","Epoch 140/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7568 - val_loss: 0.5799 - val_accuracy: 0.7559\n","Epoch 141/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7529 - val_loss: 0.5791 - val_accuracy: 0.7520\n","Epoch 142/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.7549 - val_loss: 0.6017 - val_accuracy: 0.7283\n","Epoch 143/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.7588 - val_loss: 0.6235 - val_accuracy: 0.7008\n","Epoch 144/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7588 - val_loss: 0.5802 - val_accuracy: 0.7441\n","Epoch 145/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7782 - val_loss: 0.5845 - val_accuracy: 0.7520\n","Epoch 146/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.7510 - val_loss: 0.5804 - val_accuracy: 0.7441\n","Epoch 147/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7665 - val_loss: 0.6617 - val_accuracy: 0.6850\n","Epoch 148/150\n","52/52 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7724 - val_loss: 0.5774 - val_accuracy: 0.7480\n","Epoch 149/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7704 - val_loss: 0.6234 - val_accuracy: 0.7087\n","Epoch 150/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5387 - accuracy: 0.7257 - val_loss: 0.5866 - val_accuracy: 0.7717\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f95b4b7d9d0>"]},"metadata":{},"execution_count":2}],"source":["# MLP with manual validation set\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","import numpy\n","\n","# fix random seed for reproducibility\n","seed = 7\n","numpy.random.seed(seed)\n","\n","# load pima indians dataset\n","dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n","\n","# split into input (X) and output (Y) variables\n","X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","# split into 67% for train and 33% for test\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n","\n","# create model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation= 'relu' ))\n","model.add(Dense(8, activation= 'relu' ))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","\n","# Compile model\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","\n","# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"]},{"cell_type":"markdown","source":["## Manual k-Fold Cross-Validation\n","Splitting the **training dataset into k subsets** and takes turns training models on all subsets except one which is held out, and evaluating model performance on the held out validation dataset. The process is **repeated** until all subsets are given an opportunity to be the held out validation set. The performance measure is then **averaged** across all models that are created.\n","\n","**StratifiedKFold** class from the scikit-learn Python\n","machine learning library to **split up the training dataset into 10 folds**. The folds are stratified, meaning that the algorithm attempts to balance the number of instances of each class in each fold."],"metadata":{"id":"Xa0zbGcbudo3"}},{"cell_type":"code","source":["# MLP for Pima Indians Dataset with 10-fold cross validation\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import StratifiedKFold\n","import numpy\n","\n","# fix random seed for reproducibility\n","seed = 7\n","numpy.random.seed(seed)\n","\n","# load pima indians dataset\n","dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n","\n","# split into input (X) and output (Y) variables\n","X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","# define 10-fold cross validation test harness\n","kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n","cvscores = []\n","for train, test in kfold.split(X, Y):\n","\n","  # create model\n","  model = Sequential()\n","  model.add(Dense(12, input_dim=8, activation= 'relu' ))\n","  model.add(Dense(8, activation= 'relu' ))\n","  model.add(Dense(1, activation= 'sigmoid' ))\n","\n","  # Compile model\n","  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","\n","  # Fit the model\n","  model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n","\n","  # evaluate the model\n","  scores = model.evaluate(X[test], Y[test], verbose=0)\n","  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","  cvscores.append(scores[1] * 100)\n","print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLYjGs7ju13J","executionInfo":{"status":"ok","timestamp":1653558333457,"user_tz":-480,"elapsed":164621,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"5aaab7d5-8a5a-4b0e-bcb1-ef4a7ada11ed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 70.13%\n","accuracy: 66.23%\n","accuracy: 75.32%\n","WARNING:tensorflow:5 out of the last 3910 calls to <function Model.make_test_function.<locals>.test_function at 0x7f95b55454d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 76.62%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f95b6ae4830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 68.83%\n","accuracy: 62.34%\n","accuracy: 79.22%\n","accuracy: 72.73%\n","accuracy: 75.00%\n","accuracy: 78.95%\n","72.54% (+/- 5.29%)\n"]}]}]}