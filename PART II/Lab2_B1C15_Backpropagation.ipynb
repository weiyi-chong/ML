{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab2_B1C15_Backpropagation.ipynb","provenance":[],"authorship_tag":"ABX9TyO5XVmsLmNhfJ76D1Jos+Bs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Backpropagation\n","Backpropagation algorithm is the classical feedforward artificial neural network. \n","Use to train large deep learning networks.\n","\n","After completing this tutorial, you will know:\n","\n","*   How to forward-propagate an input to calculate an output.\n","*   How to backpropagate error and train a network.\n","*   How to apply the backpropagation algorithm to a real-world predictive modeling problem.\n","\n","Backpropagation algorithm is a supervised learning method for multilayer feedforward networks from the field of Artificial Neural Network. The principle of the backpropagation approach is to model a given function by modifying\n","internal weightings of input signals to produce an expected output signal."],"metadata":{"id":"tpXLqE50cjLo"}},{"cell_type":"markdown","source":["### 15.2.1 Initialize Network\n","The creation of a new network ready for training.\n","\n","**initialize_network()** creates a new neural network ready\n","for training. It accepts three parameters: the number of inputs, the number of neurons to have\n","in the hidden layer and the number of outputs.\n"],"metadata":{"id":"gn7Av0PGea7A"}},{"cell_type":"code","source":["# Example of initializing a network\n","from random import seed\n","from random import random\n","\n","# Function To Initialize a Multilayer Perceptron Network.\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","  network = list()\n","  hidden_layer = [{ 'weights' :[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","  network.append(hidden_layer)\n","  output_layer = [{ 'weights' :[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","  network.append(output_layer)\n","  return network\n","\n","# Test initializing a network\n","seed(1)\n","network = initialize_network(2, 1, 2)\n","for layer in network:\n","  print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c01j-BdIjfc8","executionInfo":{"status":"ok","timestamp":1653556465732,"user_tz":-480,"elapsed":542,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"cf14bf15-06c8-440e-af06-0293a6d0e5f9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n","[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"]}]},{"cell_type":"markdown","source":["### 15.2.2 Forward-Propagate\n","Calculate an output from a neural network by propagating an input signal through\n","each layer until the output layer outputs its values.\n","\n","This technique is used to generate predictions during training that will need to be corrected and it is the method we will nedd after the network is trained to make predictions on new data"],"metadata":{"id":"YX8e9qYekJ3B"}},{"cell_type":"markdown","source":["#### Part 1: Neuron Activation\n","Calculate the the activation of one neuron given an input"],"metadata":{"id":"KWrrSd79knPN"}},{"cell_type":"code","source":["# Calculate neuron activation for an input\n","# Function To Activate a Neuron.\n","def activate(weights, inputs):\n","  activation = weights[-1]\n","  for i in range(len(weights)-1):\n","    activation += weights[i] * inputs[i]\n","  return activation"],"metadata":{"id":"dLgMMl9qk3ii","executionInfo":{"status":"ok","timestamp":1653556466140,"user_tz":-480,"elapsed":15,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### Part 2: Neuron Transfer\n","After neuron is activated, the activation needs to be transferred to see what the neuron output actually is. Different transfer functions can be used.\n","\n","\n","1.   Sigmoid activation function/ logistic function (traditional) - S-shaped\n","2.   Tanh (hyperbolic tangent) funtion\n","3.   Rectifier transfer function\n","\n"],"metadata":{"id":"RI3p221ClUOt"}},{"cell_type":"code","source":["from math import exp\n","\n","# Function to Transfer neuron activation\n","def transfer(activation):\n","  return 1.0 / (1.0 + exp(-activation))"],"metadata":{"id":"MwrfM_ykl9tH","executionInfo":{"status":"ok","timestamp":1653556466141,"user_tz":-480,"elapsed":11,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#### Part 3: Forward-Propagation\n","**forward_propagate()** implements the forward-propagation for a row of data from our dataset with our neural network\n","\n","We collect the outputs for a layer in an array named **new_inputs** that\n","becomes the array inputs and is used as inputs for the following layer. \n","\n","The function returns the outputs from the last layer also called the output layer."],"metadata":{"id":"Rc57Oi7XmN2G"}},{"cell_type":"code","source":["# Forward-propagate input to a network output\n","# Function To Forward-Propagate Input Through a Network.\n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = []\n","    for neuron in layer:\n","      activation = activate(neuron[ 'weights' ], inputs)\n","      neuron[ 'output' ] = transfer(activation)\n","      new_inputs.append(neuron[ 'output' ])\n","    inputs = new_inputs\n","  return inputs"],"metadata":{"id":"GYcIywQ9oNAx","executionInfo":{"status":"ok","timestamp":1653556466142,"user_tz":-480,"elapsed":12,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#### Example of Forward-Propagating an Input Through a Network."],"metadata":{"id":"yXL5v67dosiI"}},{"cell_type":"code","source":["# Example of forward propagating input\n","from math import exp\n","\n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","  activation = weights[-1]\n","  for i in range(len(weights)-1):\n","    activation += weights[i] * inputs[i]\n","  return activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","  return 1.0 / (1.0 + exp(-activation))\n","  \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = []\n","    for neuron in layer:\n","      activation = activate(neuron[ 'weights' ], inputs)\n","      neuron[ 'output' ] = transfer(activation)\n","      new_inputs.append(neuron[ 'output' ])\n","    inputs = new_inputs\n","  return inputs\n","\n","# test forward propagation\n","network = [[{ 'weights' : [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","          [{ 'weights' : [0.2550690257394217, 0.49543508709194095]}, \n","           { 'weights' : [0.4494910647887381, 0.651592972722763]}]]\n","row = [1, 0, None]\n","output = forward_propagate(network, row)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dESgli1xpGy5","executionInfo":{"status":"ok","timestamp":1653556466142,"user_tz":-480,"elapsed":11,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"1855ab7f-b05f-41fa-cdd1-aca4dfa785d3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6629970129852887, 0.7253160725279748]\n"]}]},{"cell_type":"markdown","source":["### 15.2.3 Backpropagate Error\n","Error iscalculated between the expected outputs and the outputs forward-propagated from the network.These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go."],"metadata":{"id":"6OYXq8K4q1cp"}},{"cell_type":"markdown","source":["#### Part 1: Transfer Derivative\n","Given an output value from a neuron, we need to calculate it’s slope. We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n","\n"," *derivative = output *(1.0 − output)* "],"metadata":{"id":"QW3zpjE1rHIY"}},{"cell_type":"code","source":["# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","  return output * (1.0 - output)"],"metadata":{"id":"uYYkVa4zrbkY","executionInfo":{"status":"ok","timestamp":1653556466143,"user_tz":-480,"elapsed":10,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#### Part 2: Error Backpropagating\n","The error for a given neuron can be calculated as follows:\n","\n","*error = (expected − output) * transfer_derivative(output)*\n","\n","The backpropagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n","\n","*error = (weightk ⇥ errorj) * transfer derivative(output)*"],"metadata":{"id":"SbkI3zeKrXMK"}},{"cell_type":"code","source":["# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","  for i in reversed(range(len(network))):\n","    layer = network[i]\n","    errors = list()\n","    if i != len(network)-1:\n","      for j in range(len(layer)):\n","        error = 0.0\n","        for neuron in network[i + 1]:\n","          error += (neuron[ 'weights' ][j] * neuron[ 'delta' ])\n","        errors.append(error)\n","    else:\n","      for j in range(len(layer)):\n","        neuron = layer[j]\n","        errors.append(expected[j] - neuron[ output ])\n","    for j in range(len(layer)):\n","      neuron = layer[j]\n","      neuron[ 'delta' ] = errors[j] * transfer_derivative(neuron[ output ]) "],"metadata":{"id":"qH1eZmkzrcHQ","executionInfo":{"status":"ok","timestamp":1653556466144,"user_tz":-480,"elapsed":10,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#### Example of backpropagate error"],"metadata":{"id":"ktYIh2Tos664"}},{"cell_type":"code","source":["# Example of backpropagating error\n","\n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","  return output * (1.0 - output)\n","\n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","  for i in reversed(range(len(network))):\n","    layer = network[i]\n","    errors = list()\n","    if i != len(network)-1:\n","      for j in range(len(layer)):\n","        error = 0.0\n","        for neuron in network[i + 1]:\n","          error += (neuron[ 'weights' ][j] * neuron[ 'delta' ])\n","        errors.append(error)\n","    else:\n","      for j in range(len(layer)):\n","        neuron = layer[j]\n","        errors.append(expected[j] - neuron[ 'output' ])\n","    for j in range(len(layer)):\n","      neuron = layer[j]\n","      neuron[ 'delta' ] = errors[j] * transfer_derivative(neuron[ 'output' ])\n","\n","# test backpropagation of error\n","network = [[{ 'output' : 0.7105668883115941, 'weights' : [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","          [{ 'output' : 0.6213859615555266, 'weights' : [0.2550690257394217, 0.49543508709194095]},\n","          { 'output' : 0.6573693455986976, 'weights' : [0.4494910647887381, 0.651592972722763]}]]\n","expected = [0, 1]\n","backward_propagate_error(network, expected)\n","for layer in network:\n","  print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqx-X3yttClt","executionInfo":{"status":"ok","timestamp":1653556466696,"user_tz":-480,"elapsed":562,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"2d10818f-1fe3-4749-8227-a6ecfa76cb1f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n","[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"]}]},{"cell_type":"markdown","source":["### 15.2.4 Train Network"],"metadata":{"id":"rSu76dobui0f"}},{"cell_type":"markdown","source":["#### Part 1: Update Weights"],"metadata":{"id":"JTYpXa9-umx1"}},{"cell_type":"code","source":["# Update network weights with error\n","def update_weights(network, row, l_rate):\n","  for i in range(len(network)):\n","    inputs = row[:-1]\n","    if i != 0:\n","      inputs = [neuron[ 'output' ] for neuron in network[i - 1]]\n","    for neuron in network[i]:\n","      for j in range(len(inputs)):\n","        neuron[ 'weights' ][j] += l_rate * neuron[ 'delta' ] * inputs[j]\n","      neuron[ 'weights' ][-1] += l_rate * neuron[ 'delta' ]"],"metadata":{"id":"iklO0e_iuyHh","executionInfo":{"status":"ok","timestamp":1653556466697,"user_tz":-480,"elapsed":14,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### Part 2: Train Networks"],"metadata":{"id":"Z5SzZ_sRusdK"}},{"cell_type":"code","source":["# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","  for epoch in range(n_epoch):\n","    sum_error = 0\n","    for row in train:\n","      outputs = forward_propagate(network, row)\n","      expected = [0 for i in range(n_outputs)]\n","      expected[row[-1]] = 1\n","      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","      backward_propagate_error(network, expected)\n","      update_weights(network, row, l_rate)\n","    print( '>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"],"metadata":{"id":"aSmGif1lvFEd","executionInfo":{"status":"ok","timestamp":1653556466698,"user_tz":-480,"elapsed":14,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#### Example of Training a Network on the Contrived Dataset"],"metadata":{"id":"-A56eWpMvgs3"}},{"cell_type":"code","source":["# Example of initializing a network by backpropagation\n","from math import exp\n","from random import seed\n","from random import random\n","\n","# Initialize a Multilayer Perceptron Network.\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","  network = list()\n","  hidden_layer = [{ 'weights' :[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","  network.append(hidden_layer)\n","  output_layer = [{ 'weights' :[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","  network.append(output_layer)\n","  return network\n","\n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","  activation = weights[-1]\n","  for i in range(len(weights)-1):\n","    activation += weights[i] * inputs[i]\n","  return activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","  return 1.0 / (1.0 + exp(-activation))\n","\n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = []\n","    for neuron in layer:\n","      activation = activate(neuron[ 'weights' ], inputs)\n","      neuron[ 'output' ] = transfer(activation)\n","      new_inputs.append(neuron[ 'output' ])\n","    inputs = new_inputs\n","  return inputs\n","\n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","  return output * (1.0 - output)\n","\n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","  for i in reversed(range(len(network))):\n","    layer = network[i]\n","    errors = list()\n","    if i != len(network)-1:\n","      for j in range(len(layer)):\n","        error = 0.0\n","        for neuron in network[i + 1]:\n","          error += (neuron[ 'weights' ][j] * neuron[ 'delta' ])\n","        errors.append(error)\n","    else:\n","      for j in range(len(layer)):\n","        neuron = layer[j]\n","        errors.append(expected[j] - neuron[ 'output' ])\n","    for j in range(len(layer)):\n","      neuron = layer[j]\n","      neuron[ 'delta' ] = errors[j] * transfer_derivative(neuron[ 'output' ])\n","\n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","  for i in range(len(network)):\n","    inputs = row[:-1]\n","    if i != 0:\n","      inputs = [neuron[ 'output' ] for neuron in network[i - 1]]\n","    for neuron in network[i]:\n","      for j in range(len(inputs)):\n","        neuron[ 'weights' ][j] += l_rate * neuron[ 'delta' ] * inputs[j]\n","      neuron[ 'weights' ][-1] += l_rate * neuron[ 'delta' ]\n","\n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","  for epoch in range(n_epoch):\n","    sum_error = 0\n","    for row in train:\n","      outputs = forward_propagate(network, row)\n","      expected = [0 for i in range(n_outputs)]\n","      expected[row[-1]] = 1\n","      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","      backward_propagate_error(network, expected)\n","      update_weights(network, row, l_rate)\n","    print( '>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n","\n","# Test training backprop algorithm\n","seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","[1.465489372,2.362125076,0],\n","[3.396561688,4.400293529,0],\n","[1.38807019,1.850220317,0],\n","[3.06407232,3.005305973,0],\n","[7.627531214,2.759262235,1],\n","[5.332441248,2.088626775,1],\n","[6.922596716,1.77106367,1],\n","[8.675418651,-0.242068655,1],\n","[7.673756466,3.508563011,1]]\n","n_inputs = len(dataset[0]) - 1\n","n_outputs = len(set([row[-1] for row in dataset]))\n","network = initialize_network(n_inputs, 2, n_outputs)\n","train_network(network, dataset, 0.5, 20, n_outputs)\n","for layer in network:\n","  print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqDEi7E3vnj0","executionInfo":{"status":"ok","timestamp":1653556466699,"user_tz":-480,"elapsed":14,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"8c077cbd-41dd-4886-b1e8-2e9ab2f7aafe"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":[">epoch=0, lrate=0.500, error=6.350\n",">epoch=1, lrate=0.500, error=5.531\n",">epoch=2, lrate=0.500, error=5.221\n",">epoch=3, lrate=0.500, error=4.951\n",">epoch=4, lrate=0.500, error=4.519\n",">epoch=5, lrate=0.500, error=4.173\n",">epoch=6, lrate=0.500, error=3.835\n",">epoch=7, lrate=0.500, error=3.506\n",">epoch=8, lrate=0.500, error=3.192\n",">epoch=9, lrate=0.500, error=2.898\n",">epoch=10, lrate=0.500, error=2.626\n",">epoch=11, lrate=0.500, error=2.377\n",">epoch=12, lrate=0.500, error=2.153\n",">epoch=13, lrate=0.500, error=1.953\n",">epoch=14, lrate=0.500, error=1.774\n",">epoch=15, lrate=0.500, error=1.614\n",">epoch=16, lrate=0.500, error=1.472\n",">epoch=17, lrate=0.500, error=1.346\n",">epoch=18, lrate=0.500, error=1.233\n",">epoch=19, lrate=0.500, error=1.132\n","[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n","[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"]}]},{"cell_type":"markdown","source":["###15.2.5 Predict\n","We can use the output values themselves directly as the probability of a pattern\n","belonging to each output class."],"metadata":{"id":"omzFjTCAxCsk"}},{"cell_type":"code","source":["# Make a prediction with a network\n","def predict(network, row):\n","  outputs = forward_propagate(network, row)\n","  return outputs.index(max(outputs))"],"metadata":{"id":"r7R4JqdsxLhf","executionInfo":{"status":"ok","timestamp":1653556466700,"user_tz":-480,"elapsed":14,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Example of making predictions\n","from math import exp\n","\n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","  activation = weights[-1]\n","  for i in range(len(weights)-1):\n","    activation += weights[i] * inputs[i]\n","  return activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","  return 1.0 / (1.0 + exp(-activation))\n","\n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = []\n","    for neuron in layer:\n","      activation = activate(neuron[ 'weights' ], inputs)\n","      neuron[ 'output' ] = transfer(activation)\n","      new_inputs.append(neuron[ 'output' ])\n","    inputs = new_inputs\n","  return inputs\n","\n","# Make a prediction with a network\n","def predict(network, row):\n","  outputs = forward_propagate(network, row)\n","  return outputs.index(max(outputs))\n","\n","# Test making predictions with the network\n","dataset = [[2.7810836,2.550537003,0],\n","[1.465489372,2.362125076,0],\n","[3.396561688,4.400293529,0],\n","[1.38807019,1.850220317,0],\n","[3.06407232,3.005305973,0],\n","[7.627531214,2.759262235,1],\n","[5.332441248,2.088626775,1],\n","[6.922596716,1.77106367,1],\n","[8.675418651,-0.242068655,1],\n","[7.673756466,3.508563011,1]]\n","network = [[{ 'weights' : [-1.482313569067226, 1.8308790073202204, 1.078381922048799]},\n","            { 'weights' : [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n","           [{ 'weights' : [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, \n","            { 'weights' : [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n","for row in dataset:\n","  prediction = predict(network, row)\n","  print( 'Expected=%d, Got=%d' % (row[-1], prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5HURPifxVfK","executionInfo":{"status":"ok","timestamp":1653556466701,"user_tz":-480,"elapsed":15,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"6dfc603d-a821-4445-808d-2fb14bdf8718"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n"]}]},{"cell_type":"markdown","source":["### 15.2.6 Wheat Seeds Case Study"],"metadata":{"id":"9AlayJXSyAMc"}},{"cell_type":"code","source":["# Backprop on the Seeds Dataset\n","from random import seed\n","from random import randrange\n","from random import random\n","from csv import reader\n","from math import exp\n","\n","# Load a CSV file\n","def load_csv(filename):\n","  dataset = list()\n","  with open(filename, 'r' ) as file:\n","    csv_reader = reader(file)\n","    for row in csv_reader:\n","      if not row:\n","        continue\n","      dataset.append(row)\n","  return dataset\n","\n","# Convert string column to float\n","def str_column_to_float(dataset, column):\n","  for row in dataset:\n","    row[column] = float(row[column].strip())\n","\n","# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","  class_values = [row[column] for row in dataset]\n","  unique = set(class_values)\n","  lookup = dict()\n","  for i, value in enumerate(unique):\n","    lookup[value] = i\n","  for row in dataset:\n","    row[column] = lookup[row[column]]\n","  return lookup\n","\n","# Find the min and max values for each column\n","def dataset_minmax(dataset):\n","  return [[min(column), max(column)] for column in zip(*dataset)]\n","\n","# Rescale dataset columns to the range 0-1\n","def normalize_dataset(dataset, minmax):\n","  for row in dataset:\n","    for i in range(len(row)-1):\n","      row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n","\n","# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","  dataset_split = list()\n","  dataset_copy = list(dataset)\n","  fold_size = int(len(dataset) / n_folds)\n","  for _ in range(n_folds):\n","    fold = list()\n","    while len(fold) < fold_size:\n","      index = randrange(len(dataset_copy))\n","      fold.append(dataset_copy.pop(index))\n","    dataset_split.append(fold)\n","  return dataset_split\n","\n","# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","  correct = 0\n","  for i in range(len(actual)):\n","    if actual[i] == predicted[i]:\n","      correct += 1\n","  return correct / float(len(actual)) * 100.0\n","\n","# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","  folds = cross_validation_split(dataset, n_folds)\n","  scores = list()\n","  for fold in folds:\n","    train_set = list(folds)\n","    train_set.remove(fold)\n","    train_set = sum(train_set, [])\n","    test_set = list()\n","    for row in fold:\n","      row_copy = list(row)\n","      test_set.append(row_copy)\n","      row_copy[-1] = None\n","    predicted = algorithm(train_set, test_set, *args)\n","    actual = [row[-1] for row in fold]\n","    accuracy = accuracy_metric(actual, predicted)\n","    scores.append(accuracy)\n","  return scores\n","\n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","  activation = weights[-1]\n","  for i in range(len(weights)-1):\n","    activation += weights[i] * inputs[i]\n","  return activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","  return 1.0 / (1.0 + exp(-activation))\n","\n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = []\n","    for neuron in layer:\n","      activation = activate(neuron[ 'weights' ], inputs)\n","      neuron[ 'output' ] = transfer(activation)\n","      new_inputs.append(neuron[ 'output' ])\n","    inputs = new_inputs\n","  return inputs\n","\n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","  return output * (1.0 - output)\n","\n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","  for i in reversed(range(len(network))):\n","    layer = network[i]\n","    errors = list()\n","    if i != len(network)-1:\n","      for j in range(len(layer)):\n","        error = 0.0\n","        for neuron in network[i + 1]:\n","          error += (neuron[ 'weights' ][j] * neuron[ 'delta' ])\n","        errors.append(error)\n","    else:\n","      for j in range(len(layer)):\n","        neuron = layer[j]\n","        errors.append(expected[j] - neuron[ 'output' ])\n","    for j in range(len(layer)):\n","      neuron = layer[j]\n","      neuron[ 'delta' ] = errors[j] * transfer_derivative(neuron[ 'output' ])\n","\n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","  for i in range(len(network)):\n","    inputs = row[:-1]\n","    if i != 0:\n","      inputs = [neuron[ 'output' ] for neuron in network[i - 1]]\n","    for neuron in network[i]:\n","      for j in range(len(inputs)):\n","        neuron[ 'weights' ][j] += l_rate * neuron[ 'delta' ] * inputs[j]\n","        neuron[ 'weights' ][-1] += l_rate * neuron[ 'delta' ]\n","\n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","  for _ in range(n_epoch):\n","    for row in train:\n","      forward_propagate(network, row)\n","      expected = [0 for i in range(n_outputs)]\n","      expected[row[-1]] = 1\n","      backward_propagate_error(network, expected)\n","      update_weights(network, row, l_rate)\n","\n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","  network = list()\n","  hidden_layer = [{ 'weights' :[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","  network.append(hidden_layer)\n","  output_layer = [{ 'weights' :[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","  network.append(output_layer)\n","  return network\n","\n","# Make a prediction with a network\n","def predict(network, row):\n","  outputs = forward_propagate(network, row)\n","  return outputs.index(max(outputs))\n","\n","# Backpropagation Algorithm With Stochastic Gradient Descent\n","def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n","  n_inputs = len(train[0]) - 1\n","  n_outputs = len(set([row[-1] for row in train]))\n","  network = initialize_network(n_inputs, n_hidden, n_outputs)\n","  train_network(network, train, l_rate, n_epoch, n_outputs)\n","  predictions = list()\n","  for row in test:\n","    prediction = predict(network, row)\n","    predictions.append(prediction)\n","  return(predictions)\n","\n","# Test Backprop on Seeds dataset\n","seed(1)\n","\n","# load and prepare data\n","filename = 'seeds_dataset.csv'\n","dataset = load_csv(filename)\n","for i in range(len(dataset[0])-1):\n","  str_column_to_float(dataset, i)\n","\n","# convert class column to integers\n","str_column_to_int(dataset, len(dataset[0])-1)\n","\n","# normalize input variables\n","minmax = dataset_minmax(dataset)\n","normalize_dataset(dataset, minmax)\n","\n","# evaluate algorithm\n","n_folds = 5\n","l_rate = 0.3\n","n_epoch = 500\n","n_hidden = 5\n","scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n","print( 'Scores: %s' % scores)\n","print( 'Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHVtT-LAyFNc","executionInfo":{"status":"ok","timestamp":1653556486188,"user_tz":-480,"elapsed":19497,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"bf1690db-5c30-4557-a387-951730322194"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Scores: [95.23809523809523, 90.47619047619048, 95.23809523809523, 92.85714285714286, 88.09523809523809]\n","Mean Accuracy: 92.381%\n"]}]}]}