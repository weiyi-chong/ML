{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab6_B2C26_Project: Sequence Classification of Movie Reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMig7ViTzIX7e81zPZnHJFW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["##26.1 Simple LSTM for Sequence Classification"],"metadata":{"id":"NpnYG_C8ik1V"}},{"cell_type":"markdown","source":["####Step 1: Import classes and functions and seed random number generator"],"metadata":{"id":"xkSMPnHrjP0a"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"O_SCHHgpievO","executionInfo":{"status":"ok","timestamp":1656560419544,"user_tz":-480,"elapsed":2627,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"outputs":[],"source":["import numpy\n","from keras.datasets import imdb\n","from keras import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","\n","#fix random seed fro reproducibility\n","numpy.random.seed(7)\n"]},{"cell_type":"markdown","source":["####Step 2: Load and split dataset\n","Split into train(50%) and test(50%) sets"],"metadata":{"id":"9bLmCbaJjYjf"}},{"cell_type":"code","source":["#only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNhj6pdcjcsR","executionInfo":{"status":"ok","timestamp":1656560426413,"user_tz":-480,"elapsed":4723,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"052c1f9f-3592-42f9-b44a-c5e16766ac61"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["####Step 3: Left-pad sequences to all be the same length\n","truncate and pad the input sequences so that they are all the same length\n","for modeling"],"metadata":{"id":"TKxJkAVyjvXZ"}},{"cell_type":"code","source":["# truncate and pad input sequence\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen= max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"],"metadata":{"id":"jBoOdYZsj1cv","executionInfo":{"status":"ok","timestamp":1656560427769,"user_tz":-480,"elapsed":1361,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["####Step 4: Define and fit lstm model for the IMDB dataset\n","The first layer is the Embedded layer\n","that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100\n","memory units (smart neurons).\n","\n","Classification problem = Dense output layer with 1 neuron and sigmoid activation function to make 0/1 predictions for two classes (good/bad)\n","\n","Binary classification problem = loss function =binary_crossentropy, ADDAM optimization algorithm\n","\n","3 epochs only = quickly overfits the problem\n","\n","batch size = 64 to space out weight updates"],"metadata":{"id":"1MOgSXpMkjFS"}},{"cell_type":"code","source":["# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation= 'sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n","model.summary()\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG5xjoZakpGH","executionInfo":{"status":"ok","timestamp":1656560554625,"user_tz":-480,"elapsed":88687,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"219d0ae1-dcde-4552-f999-98310513bedd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 500, 32)           160000    \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               53200     \n","                                                                 \n"," dense (Dense)               (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/3\n","391/391 [==============================] - 21s 36ms/step - loss: 0.7122 - accuracy: 0.5965 - val_loss: 0.6427 - val_accuracy: 0.6292\n","Epoch 2/3\n","391/391 [==============================] - 14s 35ms/step - loss: 0.4795 - accuracy: 0.7704 - val_loss: 0.5763 - val_accuracy: 0.7342\n","Epoch 3/3\n","391/391 [==============================] - 15s 39ms/step - loss: 0.3295 - accuracy: 0.8639 - val_loss: 0.3271 - val_accuracy: 0.8635\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6e11df3350>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["####Step 5: Evaluate the Fit model"],"metadata":{"id":"qBoBK22llVL6"}},{"cell_type":"code","source":["scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFVy44ojlY5t","executionInfo":{"status":"ok","timestamp":1656560560868,"user_tz":-480,"elapsed":6259,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"6554bb76-0aa5-43d8-b1c7-30bc5345d3ce"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 86.35%\n"]}]},{"cell_type":"markdown","source":["####FULL CODE"],"metadata":{"id":"JXeqkyv5lsok"}},{"cell_type":"code","source":["# LSTM for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","model.summary()\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJkECjWFlwaB","executionInfo":{"status":"ok","timestamp":1656560615071,"user_tz":-480,"elapsed":54217,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"45a2b3d8-879f-41d1-be8e-81a707e2c206"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 500, 32)           160000    \n","                                                                 \n"," lstm_1 (LSTM)               (None, 100)               53200     \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/3\n","391/391 [==============================] - 12s 26ms/step - loss: 0.5051 - accuracy: 0.7495\n","Epoch 2/3\n","391/391 [==============================] - 10s 25ms/step - loss: 0.3367 - accuracy: 0.8639\n","Epoch 3/3\n","391/391 [==============================] - 10s 26ms/step - loss: 0.2700 - accuracy: 0.8933\n","Accuracy: 86.00%\n"]}]},{"cell_type":"markdown","source":["##26.2 LSTM for Sequence Classification with Dropout\n","Recurrent Neural networks like LSTM generally have the problem of overfitting. Dropout can\n","be applied between layers using the Dropout Keras layer.\n","\n","Add new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers"],"metadata":{"id":"I5XdYxWMl__v"}},{"cell_type":"code","source":["# LSTM with Dropout for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","#***************************add Dropout\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","#***************************add Dropout\n","model.add(Dropout(0.2))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","model.summary()\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSnm6ZCfl6wN","executionInfo":{"status":"ok","timestamp":1656560658691,"user_tz":-480,"elapsed":43625,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"4df81a45-4703-4b7f-f7ce-5916a6e81965"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 500, 32)           160000    \n","                                                                 \n"," dropout (Dropout)           (None, 500, 32)           0         \n","                                                                 \n"," lstm_2 (LSTM)               (None, 100)               53200     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 100)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/3\n","391/391 [==============================] - 12s 26ms/step - loss: 0.4610 - accuracy: 0.7814\n","Epoch 2/3\n","391/391 [==============================] - 10s 26ms/step - loss: 0.3264 - accuracy: 0.8636\n","Epoch 3/3\n","391/391 [==============================] - 10s 25ms/step - loss: 0.2616 - accuracy: 0.8994\n","Accuracy: 87.68%\n"]}]},{"cell_type":"markdown","source":["Droput having desired impact on training with slightly slower trend in convergence and lower final accuracy. \n","To achieve higher skill \n","1. more epochs\n","2. dropout applied to the input and recurrent connections of the memory units with the LTSM precisely and separately\n","\n","Keras provides this capability with parameters on the LSTM layer,\n","the dropout for configuring the input dropout and recurrent dropout for configuring the\n","recurrent dropout."],"metadata":{"id":"SM-F2CODqWBF"}},{"cell_type":"code","source":["# LSTM with dropout for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","# *****************more precise LSTM droupout\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","model.summary()\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"akTpvo0gq4AY","executionInfo":{"status":"ok","timestamp":1656563050398,"user_tz":-480,"elapsed":2391722,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"84e9e6c6-caad-4112-b7c9-c2e63521869d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 500, 32)           160000    \n","                                                                 \n"," lstm_3 (LSTM)               (None, 100)               53200     \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/3\n","391/391 [==============================] - 749s 2s/step - loss: 0.5005 - accuracy: 0.7440\n","Epoch 2/3\n","391/391 [==============================] - 740s 2s/step - loss: 0.3433 - accuracy: 0.8583\n","Epoch 3/3\n","391/391 [==============================] - 736s 2s/step - loss: 0.3063 - accuracy: 0.8710\n","Accuracy: 85.41%\n"]}]},{"cell_type":"markdown","source":["##26.3 LSTM and CNN for Sequence Classification\n","We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small\n","filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map\n","size."],"metadata":{"id":"p6CqSabFrTo2"}},{"cell_type":"code","source":["# LSTM and CNN for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","# ************add 1-d CNN and max pooling layers\n","model.add(Conv1D(filters=32, kernel_size=3, padding= 'same' , activation= 'relu' ))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(LSTM(100))\n","model.add(Dense(1, activation= 'sigmoid' ))\n","model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n","model.summary()\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BwCtdZZirfC7","executionInfo":{"status":"ok","timestamp":1656563126590,"user_tz":-480,"elapsed":52616,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"fc8a9e9f-cd63-406f-bae8-ba143c87f183"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, 500, 32)           160000    \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 500, 32)           3104      \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 250, 32)          0         \n"," 1D)                                                             \n","                                                                 \n"," lstm_5 (LSTM)               (None, 100)               53200     \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 216,405\n","Trainable params: 216,405\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/3\n","391/391 [==============================] - 15s 16ms/step - loss: 0.4431 - accuracy: 0.7739\n","Epoch 2/3\n","391/391 [==============================] - 6s 16ms/step - loss: 0.2532 - accuracy: 0.8995\n","Epoch 3/3\n","391/391 [==============================] - 6s 16ms/step - loss: 0.2008 - accuracy: 0.9230\n","Accuracy: 88.37%\n"]}]}]}